{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader,SequentialSampler # RandomSampler\n",
    "\n",
    "from sklearn. metrics import recall_score,precision_score, f1_score,plot_precision_recall_curve,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import *\n",
    "from untils import *\n",
    "from model import *\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lrD\", type=float, default=0.0005, help=\"discriminator adam: learning rate\")\n",
    "parser.add_argument(\"--lrG\", type=float, default=0.0005, help=\"generator adam: learning rate\")\n",
    "parser.add_argument(\"--lrC\", type=float, default=0.001, help=\"classifier adam: learning rate\")\n",
    "parser.add_argument(\"--wdG\", type=float, default=5e-4, help=\"generator adam: weight_decay\")\n",
    "parser.add_argument(\"--wdD\", type=float, default=5e-4, help=\"discriminator adam: weight_decay\")\n",
    "parser.add_argument(\"--wdC\", type=float, default=5e-4, help=\" classifier adam: weight_decay\")\n",
    "parser.add_argument(\"--p_hint\", type=float, default=0.5, help=\"model: probability of hint\")\n",
    "parser.add_argument(\"--k\", type=int, default=5, help=\"model:k for G&C running per iteration\")\n",
    "parser.add_argument(\"--alpha\", type=int, default=5, help=\"model: alpha parameter reconstructed loss\")\n",
    "parser.add_argument(\"--beta\", type=int, default=1, help=\"model: beta parameter classifier loss\")\n",
    "parser.add_argument(\"--n_class\", type=int, default=1, help=\"label class\")\n",
    "parser.add_argument('--H_Dim1', type=int, default=32, help='hiden layer dimension1')\n",
    "parser.add_argument('--H_Dim2', type=int, default=32, help='hiden layer dimension2')\n",
    "parser.add_argument('--H_Dim3', type=int, default=64, help='hiden layer dimension3')\n",
    "parser.add_argument(\"--dropoutD\", type=float, default=0.1, help=\"dropout: NetD dropout\")\n",
    "parser.add_argument(\"--dropoutG\", type=float, default=0.1, help=\"dropout: NetD dropout\")\n",
    "parser.add_argument(\"--dropoutC\", type=float, default=0.1, help=\"dropout: NetD dropout\")\n",
    "parser.add_argument(\"--file_x\", type=str, default=\"6hr\", help=\"file: file feature name\")\n",
    "parser.add_argument(\"--file_m\", type=str, default=\"6hr20\", help=\"file: file mask name\")\n",
    "parser.add_argument(\"--file_y\", type=str, default=\"6hr\", help=\"file: file label name\")\n",
    "parser.add_argument(\"--thold\", type=float, default=0.5, help=\"logits threshold\")\n",
    "opt, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.seed(321)\n",
    "np.random.seed(321)\n",
    "torch.manual_seed(321)\n",
    "torch.cuda.manual_seed(321)\n",
    "torch.cuda.manual_seed_all(321)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy, devX, devy, testX, testy, trainX_M,testX_M,devX_M=data_load(opt.file_x,opt.file_y, opt.file_m,mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "no,dim=trainX.shape\n",
    "netG = NetG(Dim=dim,H_Dim=opt.H_Dim1,dropout=opt.dropoutG)\n",
    "netD = NetD(Dim=dim,H_Dim=opt.H_Dim2,n_class=opt.n_class,dropout=opt.dropoutD)\n",
    "netC = NetC(Dim=dim,H_Dim=opt.H_Dim3,n_class=opt.n_class, dropout=opt.dropoutC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if use_cuda:\n",
    "    netD = netD.cuda()\n",
    "    netG = netG.cuda()\n",
    "    netC = netC.cuda()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert all data to tensor type\n",
    "trainX_tensor = torch.tensor(trainX).float()\n",
    "trainy_tensor = torch.tensor(trainy).float()\n",
    "trainM_tensor = torch.tensor(trainX_M).float()\n",
    "\n",
    "\n",
    "devX_tensor = torch.tensor(devX).float()\n",
    "devy_tensor = torch.tensor(devy).float()\n",
    "devM_tensor = torch.tensor(devX_M).float()\n",
    "\n",
    "testX_tensor = torch.tensor(testX).float()\n",
    "testy_tensor = torch.tensor(testy).float()\n",
    "testM_tensor = torch.tensor(testX_M).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimD = torch.optim.Adam(netD.parameters(), lr=opt.lrD, weight_decay=opt.wdD) \n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=opt.lrG,weight_decay=opt.wdG)\n",
    "optimC = torch.optim.Adam(netC.parameters(), lr=opt.lrC,weight_decay=opt.wdC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "train_dataset = TensorDataset(trainX_tensor, trainM_tensor, trainy_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=opt.batch_size) \n",
    "\n",
    "dev_dataset = TensorDataset(devX_tensor,  devM_tensor, devy_tensor)\n",
    "dev_sampler = SequentialSampler(dev_dataset)\n",
    "dev_dataloader = DataLoader(dev_dataset, sampler=dev_sampler, batch_size=opt.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/random_ops.py:285: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  minval_is_zero = minval is 0  # pylint: disable=literal-comparison\n",
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/random_ops.py:286: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  maxval_is_one = maxval is 1  # pylint: disable=literal-comparison\n",
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/ragged/ragged_batch_gather_with_default_op.py:84: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if (default_value.shape.ndims is not 0\n",
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/ragged/ragged_batch_gather_with_default_op.py:85: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  and default_value.shape.ndims is not 1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6630665229304227\n",
      "0.692107545533391\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "auc_max=0\n",
    "f1_max=0\n",
    "accumulation_step=opt.k\n",
    "global_step=0\n",
    "for epoch_num in range(opt.n_epochs):\n",
    "        \n",
    "    netD.train()\n",
    "    netG.train()\n",
    "    netC.train()\n",
    "    \n",
    "\n",
    "    test_loss_mse=0\n",
    "    train_loss_D=0\n",
    "    predict_loss=0\n",
    "    for step_num, batch_data in list(enumerate(train_dataloader))[:-1]:\n",
    "        global_step+=1\n",
    "        torch.cuda.empty_cache()\n",
    "        X, M,y= tuple(t.to(device) for t in batch_data)\n",
    "        B = sample_binary(X.shape[0], dim, 1-opt.p_hint)\n",
    "        B = torch.tensor(B,device=\"cuda\").float()\n",
    "        H =M *B+0.5*(1-B)\n",
    "        \n",
    "        \n",
    "        # -----------------\n",
    "        #  Train G&C\n",
    "        # -----------------\n",
    "\n",
    "        optimG.zero_grad()\n",
    "        optimC.zero_grad()\n",
    "        \n",
    "        \n",
    "        class_loss_sum=0\n",
    "        reconstruct_loss_sum=0\n",
    "        adversarial_G_loss_sum=0\n",
    "        for i in range(accumulation_step):\n",
    "            Z= Variable(FloatTensor(np.random.uniform(0, 1, (X.shape[0], dim))))\n",
    "        \n",
    "            New_X = M * X + (1-M) * Z\n",
    "            G_sample = netG(New_X, M)\n",
    "            Hat_New_X = New_X*M + G_sample * (1-M)\n",
    "            C_logits=netC(Hat_New_X)\n",
    "            D_logit = netD(Hat_New_X, H,C_logits) \n",
    "        \n",
    "            adversarial_G_loss,reconstruct_loss=generator_loss(New_X, M, G_sample, D_logit)\n",
    "#             adversarial_G_loss =(-torch.mean((1 - M) * (torch.sigmoid(D_logit)+1e-8).log())/(1-M).mean())\n",
    "#             reconstruct_loss=mse_loss(M*New_X, M*G_sample)/ torch.mean(M)\n",
    "            class_loss=bce_loss(C_logits,y)\n",
    "            \n",
    "            total_G_loss = adversarial_G_loss + opt.alpha*reconstruct_loss+class_loss*opt.beta\n",
    "\n",
    "            class_loss_sum += class_loss.item()\n",
    "            reconstruct_loss_sum += reconstruct_loss.item()\n",
    "            adversarial_G_loss_sum += adversarial_G_loss.item()    \n",
    "\n",
    "            total_G_loss.backward()\n",
    "\n",
    "        writer.add_scalar(\"train/G_Loss/adversarial_G_loss\", adversarial_G_loss_sum/accumulation_step, global_step)\n",
    "        writer.add_scalar(\"train/G_Loss/reconstruct_loss\", reconstruct_loss_sum/accumulation_step, global_step)\n",
    "        writer.add_scalar(\"train/G_Loss/class_loss\", class_loss_sum/accumulation_step, global_step)\n",
    "        \n",
    "        \n",
    "        optimG.step() \n",
    "        optimC.step() \n",
    "\n",
    "        # -----------------\n",
    "        #  Train  D\n",
    "        # -----------------\n",
    "        optimD.zero_grad()\n",
    "        \n",
    "        # update D only\n",
    "        D_logit_imputed= netD(Hat_New_X.detach(), H,C_logits.detach() )\n",
    "        D_loss= discriminator_loss(D_logit_imputed, M)\n",
    "        train_loss_D +=D_loss.item()\n",
    "        writer.add_scalar(\"train/D_Loss\", D_loss,  global_step)\n",
    "       \n",
    "        D_loss.backward() \n",
    "        optimD.step() \n",
    "          \n",
    "     \n",
    "    \n",
    "    netD.eval()\n",
    "    netG.eval()\n",
    "    netC.eval()\n",
    "\n",
    "    all_logits  = []\n",
    "    label_pred = []\n",
    "    with torch.no_grad():\n",
    "        for step_num, batch_data in enumerate(dev_dataloader):\n",
    "            dev_X, dev_M, dev_y = tuple(t.to(device) for t in batch_data)\n",
    "            dev_Z= Variable(FloatTensor(np.random.uniform(0, 1, (dev_X.shape[0], dim))))\n",
    "            dev_New_X = dev_X * dev_M + (1-dev_M) * dev_Z\n",
    "            dev_B = sample_binary(dev_X.shape[0], dim, 1-opt.p_hint)\n",
    "            \n",
    "            dev_B = torch.tensor(dev_B,device=\"cuda\").float()\n",
    "            dev_H = dev_M *dev_B+0.5*(1-dev_B)\n",
    "            \n",
    "            dev_G_sample = netG(dev_New_X,dev_M)\n",
    "            dev_Hat_New_X = dev_New_X*dev_M +dev_G_sample * (1-dev_M)\n",
    "            dev_pred_output = netC(dev_Hat_New_X)\n",
    "\n",
    "            predict_loss += bce_loss(dev_pred_output, dev_y)\n",
    "\n",
    "            numpy_logits = dev_pred_output.cpu().detach().numpy()\n",
    "            label_pred += list((numpy_logits[:, 0] > opt.thold).astype(int) )\n",
    "            all_logits += list(numpy_logits[:, 0])\n",
    "            test_loss_mse +=test_loss(dev_X,dev_M,dev_G_sample).item()\n",
    "            \n",
    "\n",
    "    predict_loss=predict_loss/(step_num+1) \n",
    "\n",
    "    \n",
    "    accuracy = np.equal(devy_tensor[:, 0], np.asarray(label_pred)).sum().detach().numpy() /  devy_tensor[:, 0].size().numel()\n",
    "\n",
    "    f1=f1_score(devy[:, 0],np.array(label_pred),zero_division=0)\n",
    "    recall=recall_score(devy[:, 0],np.array(label_pred),zero_division=0) \n",
    "    precision=precision_score(devy[:, 0],np.array(label_pred),zero_division=0)\n",
    "    auc = roc_auc_score(devy[:, 0], np.array(all_logits))\n",
    "    \n",
    "   \n",
    "    writer.add_scalar(\"devp/predict_loss\", predict_loss, epoch_num+1)\n",
    "    writer.add_scalar(\"devp/auc\", auc, epoch_num+1)\n",
    "    writer.add_scalar(\"devp/f1\", f1, epoch_num+1)\n",
    "\n",
    "\n",
    "    if f1>f1_max:\n",
    "        f1_max=f1\n",
    "    if auc>auc_max:\n",
    "        auc_max=auc\n",
    "\n",
    "    writer.close()\n",
    "print(auc_max)\n",
    "print(f1_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# impute example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(X,M):\n",
    "    Z= Variable(FloatTensor(np.random.uniform(0, 1, (X.shape[0], dim))))\n",
    "    New_X = M * X + (1-M) * Z  # Missing Data Introduce\n",
    "    G_sample = netG(New_X,M)\n",
    "    Hat_New_X=M*X +(1-M)*G_sample\n",
    "    \n",
    "    return Hat_New_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = torch.tensor(testX,device=device).float()\n",
    "testy= torch.tensor(testy,device=device).float()\n",
    "#devZ_tensor = torch.tensor(devZ).float()\n",
    "testM = torch.tensor(testX_M,device=device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgain_X=impute(trainX_tensor.to(device),trainM_tensor.to(device)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
